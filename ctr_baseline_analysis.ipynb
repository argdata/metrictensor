{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science: Click-through rate\n",
    "\n",
    "**Notebook by Emmanuel Contreras-Campana, PhD**\n",
    "\n",
    "### Introduction\n",
    "\n",
    "One of the key applications at TripleLift is a bidder designed to optimize the performance of campaigns. A big driver of this performance is the click through rate (clicks/impressions). We take a sample of company data (<a href=\"https://s3.amazonaws.com/ops.triplelift.net/public/code-test/data-code-test.tar.gz\">data</a>) and use it to develop a model that is able to predict the click through rate.\n",
    "\n",
    "The columns in the file are:\n",
    "- **timestamp**: time of the impression\n",
    "- **placement_id**: a unique identifier for a web page\n",
    "- **browser_id**: unique identifier for a browser (firefox, chrome, ie10, etc)\n",
    "- **os_id**: unique identifier for an os (windows, linux, osx)\n",
    "- **region**: geographic region (states in the US)\n",
    "- **country**: country code\n",
    "- **is_adserver**: ignore this column\n",
    "- **campaign**: unique identifier for a campaign (with it's own targeting parameters - for example target NY +\n",
    "NJ)\n",
    "- **creative_asset_id**: unique identifier for an image belonging to a campaign mouseovers: 1 if there was a mouseover\n",
    "**clicks**: 1 if thee was there a click\n",
    "- **max_duration**: if this was a video, how far did the viewer get video_length: if this was a video, what was the length of the video viewable: was the ad viewable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "\n",
    "We load all the necessary python libraries that will permit us to load the data files, pre-process and clean the data, perform data validation, produce statistical summaries, conduct exploratory data analysis, as well as feature transformation, feature ranking, and feature selection. Python libraries will also be needed for model selection, evaluating overfitting, executing standard nested k-fold cross validation for hyper-parameter optimization and model evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import common python libraries\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import heapq\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import operator\n",
    "import collections\n",
    "\n",
    "# Import panda library\n",
    "import pandas.core.common as com\n",
    "from pandas.tools import plotting\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from pandas.core.index import Index\n",
    "\n",
    "# Import scipy\n",
    "import scipy as sp\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Import itertools\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "\n",
    "# Import collections\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Import Jupyter\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import display\n",
    "\n",
    "# Import scikit-learn\n",
    "import sklearn\n",
    "\n",
    "from sklearn.preprocessing import (StandardScaler, RobustScaler, MinMaxScaler,\n",
    "                                   LabelEncoder, OneHotEncoder)\n",
    "\n",
    "from sklearn import feature_selection\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, roc_auc_score, roc_curve, \n",
    "                             auc, average_precision_score, precision_score, \n",
    "                             brier_score_loss, recall_score, f1_score, log_loss, \n",
    "                             classification_report, precision_recall_curve,\n",
    "                             accuracy_score)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Import keras library\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Import imblearn\n",
    "import imblearn\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Import data science toolkit\n",
    "from dskit import *\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "seed = 7\n",
    "random.seed(a=seed)\n",
    "\n",
    "# Specifying which nodes should be run interactively\n",
    "#InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Files\n",
    "\n",
    "Most data files contain approximately 1M entries. There are a total of 8 files totaling 8M data entries. We list the features and response names. We store the data in a Pandas DataFrame for greater ease of data manipulation.\n",
    "\n",
    "**Note: To reduce running time of the program comment out some of the input files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Load data files\n",
    "\n",
    "# Feature names\n",
    "features = ['timestamp', 'placement_id', 'browser_id', 'os_id',\n",
    "            'region', 'country', 'is_adserver', 'campaign', \n",
    "            'creative_asset_id', 'mouseovers', 'clicks', 'max_duration',\n",
    "            'video_length', 'viewable']\n",
    "\n",
    "# Check loading data with sc.textFile\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('ctr/')\n",
    "filePath = os.path.join(baseDir, inputPath)\n",
    "\n",
    "# Load dataset\n",
    "csvList = []\n",
    "\n",
    "csvList.append(pd.read_csv(filepath_or_buffer=filePath+'data-0000_part_00', \n",
    "                           header = None, delimiter = '|', names = features))\n",
    "csvList.append(pd.read_csv(filepath_or_buffer=filePath+'data-0001_part_00', \n",
    "                           header = None, delimiter = '|', names = features))\n",
    "csvList.append(pd.read_csv(filepath_or_buffer=filePath+'data-0002_part_00', \n",
    "                           header = None, delimiter = '|', names = features))\n",
    "csvList.append(pd.read_csv(filepath_or_buffer=filePath+'data-0003_part_00', \n",
    "                           header = None, delimiter = '|', names = features))\n",
    "csvList.append(pd.read_csv(filepath_or_buffer=filePath+'data-0004_part_00', \n",
    "                           header = None, delimiter = '|', names = features))\n",
    "csvList.append(pd.read_csv(filepath_or_buffer=filePath+'data-0005_part_00', \n",
    "                           header = None, delimiter = '|', names = features))\n",
    "csvList.append(pd.read_csv(filepath_or_buffer=filePath+'data-0006_part_00', \n",
    "                           header = None, delimiter = '|', names = features))\n",
    "csvList.append(pd.read_csv(filepath_or_buffer=filePath+'data-0007_part_00', \n",
    "                           header = None, delimiter = '|', names = features))\n",
    "\n",
    "df_raw_full = pd.concat(csvList)\n",
    "\n",
    "print \"Total number of events:\", df_raw_full.shape[0]                 \n",
    "print \"Number of features:\", df_raw_full.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## To decrease the running time even futher we reduce the number of rows \n",
    "## of the dataframe to 10k randomly sampled data entries\n",
    "\n",
    "full = False \n",
    "\n",
    "df_raw = df_raw_full if full==True else df_raw_full.sample(n=200000, \n",
    "                                                           replace=False, \n",
    "                                                           random_state=seed, \n",
    "                                                           axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing and cleaning\n",
    "\n",
    "We investigate the data for any missing values and decide if any feature should be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## The following snippet of code was written by Rajashree Baradur\n",
    "## https://github.com/rajashreebaradur/Kaggle_HousePrices/blob/\n",
    "## master/getting_cleaning_data.ipynb\n",
    "\n",
    "# Number of null entries\n",
    "print df_raw.select_dtypes(include=['int']).isnull().sum(), \"\\n\"\n",
    "print df_raw.select_dtypes(include=['float']).isnull().sum(), \"\\n\"\n",
    "print df_raw.select_dtypes(include=['object']).isnull().sum(), \"\\n\"\n",
    "\n",
    "# Number of non-null entries\n",
    "print \"\\nInformation:\" \n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is pre-processed and cleaned for any missing values. We drop the \"is_adserver\" column since it was asked to be ignored. The \"max_duration\" and \"video_length\" columns are dropped because there were few instances in which they had any actual values. We will retain the \"region\" and \"country\" columns because there are enough entries with actual values.\n",
    "\n",
    "The \"timestamp\" column has date and time entries from 10/25/2015. Therefore, we eliminate the extraneous date information and only retain the hour, minute, and second information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Data pre-processing and cleaning (data munging)\n",
    "\n",
    "# Drop irrelevant columns or columns with too many NaN\n",
    "df_raw_selected = df_raw.drop(['is_adserver', 'max_duration', 'video_length'], \n",
    "                              axis=1, inplace=False) \n",
    "\n",
    "# Drop rows with at least one NaN\n",
    "df_raw_selected.dropna(how='any', inplace=True)\n",
    "\n",
    "# Convert timestamp from string to pandas timestamp\n",
    "df_raw_selected['timestamp'] = df_raw_selected['timestamp'].apply(pd.Timestamp)\n",
    "\n",
    "# Extract day of week from pandas timestamp\n",
    "#df_raw_selected['day_of_week'] = \\\n",
    "#df_raw_selected['timestamp'].apply(lambda t: t.weekday_name)\n",
    "\n",
    "# Extract hour from pandas timestamp\n",
    "df_raw_selected['hour'] = df_raw_selected['timestamp'].apply(lambda t: t.hour)\n",
    "\n",
    "# Remove duplicate entries from dataframe if there are any\n",
    "df_raw_selected.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary\n",
    "\n",
    "We give a statistical summary below to make sure the data makes sense and that nothing anomolous is present. As we can see values look promising and have acceptable variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Calculate class label cpercentages\n",
    "\n",
    "composition = df_raw_selected.groupby(by='clicks', axis=0).count()['timestamp']\n",
    "\n",
    "print composition/composition.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of clicks in the data is about 0.46% which suggests that we are dealing with an extremely unbalanced dataset. In all likelihood, we will have to perform some class balancing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Print statistical summary of dataset\n",
    "\n",
    "# To print out all rows and columns to the terminal\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print \"Head:\"\n",
    "display(df_raw_selected.head())\n",
    "\n",
    "print \"\\nDescribe:\"\n",
    "display(df_raw_selected.describe())\n",
    "\n",
    "print \"Describe based on clicks or non-clicks:\"\n",
    "display(df_raw_selected.groupby('clicks').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Visualization: Basic Exploratory Data Analyses\n",
    "\n",
    "We conduct a basic exploratory data analyses by depicting the relationship between **click vs no-click**, and the features. For the investigation of the click through rate we will not make use of \"viewable\" and \"mouseovers\" as these are not generally available during real-time bidding of ad impressions. We will keep \"hour\" but drop \"timestamp\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_raw_selected.drop(['timestamp', 'viewable', 'mouseovers'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chi-squared statistic may be used to determine if any two features are independent of one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = ['placement_id', 'browser_id', 'os_id',\n",
    "           'region', 'country', 'campaign', \n",
    "           'creative_asset_id', 'clicks']\n",
    "\n",
    "plot_chi2_matrix(df_raw_selected, columns, p_value=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot bar graphs to determine if there are any trends between click through rates and any of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Plot bar graph of categorical features\n",
    "\n",
    "# features\n",
    "column=['placement_id', 'browser_id', 'os_id', 'region',\n",
    "        'country', 'campaign', 'creative_asset_id'] # 'day_of_week'\n",
    "\n",
    "# legend labels\n",
    "label = ['no-clicks', 'clicks']\n",
    "\n",
    "# signal and background\n",
    "signal = df_raw_selected.query('clicks > 0')\n",
    "background = df_raw_selected.query('clicks < 1')\n",
    "\n",
    "# plot histograms\n",
    "plot_categorical_features(signal, background, columns=column, top_n=20, \n",
    "                          normed=True, style=None, legend_label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Plot hitogram of numerical features\n",
    "\n",
    "# features\n",
    "column=['hour']\n",
    "\n",
    "# legend labels\n",
    "label = ['no-clicks', 'clicks']\n",
    "\n",
    "# signal and background\n",
    "signal = df_raw_selected.query('clicks > 0')\n",
    "background = df_raw_selected.query('clicks < 1')\n",
    "\n",
    "# plot histograms\n",
    "plot_numerical_features(signal, background, columns=column, \n",
    "                        bins=24, normed=True, style=None, \n",
    "                        discrete=True, legend_label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Since most machine learning algorithms cannot handle categorical (nominal) features directly we will perform **one-hot-encoding** and then droping them from the original dataframe to eliminate collinearity. We also create the features dataframe and response array which will later on be used for feature ranking and machine learning modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Create features dataframe and target array\n",
    "\n",
    "df_X = df_raw_selected.drop('clicks', axis=1, inplace=False)\n",
    "df_y = df_raw_selected['clicks']\n",
    "\n",
    "# Split data into training and test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "       train_test_split(df_X, df_y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Label encoding\n",
    "\n",
    "# features to label encode\n",
    "columns = ['region', 'country']\n",
    "\n",
    "# label encode\n",
    "X_train, X_test = label_encoder(X_train, X_test, columns)\n",
    "\n",
    "# drop origin \"region\" and \"country\" columns\n",
    "X_train.drop(columns, axis=1, inplace=True)\n",
    "X_test.drop(columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Feature transformations of categorical (nominal) variables\n",
    "\n",
    "# One-hot encoding\n",
    "# Note: sparse=False for numpy.ndarray \n",
    "# else sparse=True for scipy.sparse.csr.csr_matrix\n",
    "enc = OneHotEncoder(categorical_features='all', n_values='auto',\n",
    "                    sparse=True, handle_unknown='ignore') \n",
    "\n",
    "# features to one-hot encode\n",
    "column = ['placement_id', 'browser_id', 'os_id', 'region_le', \n",
    "          'country_le', 'campaign', 'creative_asset_id']\n",
    "\n",
    "enc.fit(X_train[column])\n",
    "\n",
    "X_train = enc.transform(X_train[column])\n",
    "X_test = enc.transform(X_test[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to consider **undersampling** of the dataset at this time to reduce the extreme imbalance in class labels. Due to a lack of time currently this will have to be investigated further at a later date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms for Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Predictive Modeling Candidates\n",
    "\n",
    "We investigate several machine learning models in order to establish which algorithm may be the most promising for the predicitve modeling of click through rates. A few performance measurements wil be used to help select our model, namely, **accuracy**, **log loss**, and the **area under the receiver operating characteristic (ROC) curve (AUC)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estiblish our **baseline model** that we use to compare all other candidated models against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fitting a dummy model (baseline model)\n",
    "# 'most_frequent', 'stratified'\n",
    "dc = DummyClassifier(strategy='stratified', random_state=seed) \n",
    "dc.fit(X_train, y_train)\n",
    "\n",
    "# persist model\n",
    "joblib.dump(dc, 'models/DummyClassifier.pkl')\n",
    "\n",
    "# Dummy classifier probability predictions\n",
    "dc_predict_proba = dc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate model performance with accuracy (i.e. Brier score)\n",
    "# note: original label composition was 0.996013\n",
    "score = 1 - brier_score_loss(y_test, dc_predict_proba)\n",
    "\n",
    "print \"baseline (Brier score):\", score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider **Logistic Regression** as our first candidate predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fitting a logistic regression model with ridge regression\n",
    "log = LogisticRegression(penalty='l2', C=1, random_state=seed)\n",
    "log.fit(X_train, y_train)\n",
    "\n",
    "# persist model\n",
    "joblib.dump(log, 'models/LogisticRegression.pkl')\n",
    "\n",
    "# Logistic regression classifier probability predictions\n",
    "log_predict_proba = log.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate model performance with accuracy (i.e. Brier score)\n",
    "score = 1 - brier_score_loss(y_test, log_predict_proba)\n",
    "\n",
    "print \"model (Brier score):\", score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there is a **0.5% difference** between the baseline model and our logistic regression model in terms of accuracy. For such an imbalanced dataset accuracy is not an appropriate model performance metric. Log loss provides a much more fine-grained evaluation between prediction and true value since in correct predicitions are heavily penalized compared to correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Evaluate model performance with log loss\n",
    "print \"baseline (log loss): \", log_loss(y_test, dc.predict_proba(X_test))\n",
    "print \"model (log loss): \", log_loss(y_test, log.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model performs an order of magnitude better than the baseline model using this alternative performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another possible candidate model we can investigate are a **Random Forest**, **Gradient Boosted Decision Trees**, **Decision Trees** classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fitting a random forest model\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=20, min_samples_leaf=10)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# persist model\n",
    "joblib.dump(rf, 'models/RandomForestClassifier.pkl')\n",
    "\n",
    "# Evaluate model performance with log loss\n",
    "score = log_loss(y_test, rf.predict_proba(X_test))\n",
    "\n",
    "print \"model (log loss): \", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fitting a gradient boosted model\n",
    "gt = GradientBoostingClassifier(n_estimators=50, max_depth=5, min_samples_leaf=10)\n",
    "gt.fit(X_train, y_train)\n",
    "\n",
    "# persist model\n",
    "joblib.dump(gt, 'models/GradientBoostingClassifier.pkl') \n",
    "\n",
    "# Evaluate model performance with log loss\n",
    "score = log_loss(y_test, gt.predict_proba(X_test))\n",
    "\n",
    "print \"model (log loss): \", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fitting a decision tree model\n",
    "dt = DecisionTreeClassifier(max_depth=10, min_samples_leaf=10)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# persist model\n",
    "joblib.dump(dt, 'models/DecisionTreeClassifier.pkl') \n",
    "\n",
    "# Evaluate model performance with log loss\n",
    "score = log_loss(y_test, dt.predict_proba(X_test))\n",
    "\n",
    "print \"model (log loss): \", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fitting an adaptive boosted model\n",
    "ab = AdaBoostClassifier()\n",
    "ab.fit(X_train, y_train)\n",
    "\n",
    "# persist model\n",
    "joblib.dump(ab, 'models/AdaBoostClassifier.pkl') \n",
    "\n",
    "# Evaluate model performance with log loss\n",
    "score = log_loss(y_test, ab.predict_proba(X_test))\n",
    "\n",
    "print \"model (log loss): \", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fitting a bagging model\n",
    "bg = BaggingClassifier(n_estimators=100)\n",
    "bg.fit(X_train, y_train)\n",
    "\n",
    "# persist model\n",
    "joblib.dump(bg, 'models/BaggingClassifier.pkl') \n",
    "\n",
    "# Evaluate model performance with log loss\n",
    "score = log_loss(y_test, bg.predict_proba(X_test))\n",
    "\n",
    "print \"model (log loss): \", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fitting a extra trees model\n",
    "et = ExtraTreesClassifier(min_samples_leaf=10)\n",
    "et.fit(X_train, y_train)\n",
    "\n",
    "# persist model\n",
    "joblib.dump(et, 'models/ExtraTreesClassifier.pkl') \n",
    "\n",
    "# Evaluate model performance with log loss\n",
    "score = log_loss(y_test, et.predict_proba(X_test))\n",
    "\n",
    "print \"model (log loss): \", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fitting a multinomial naive bayes model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# persist model\n",
    "joblib.dump(mnb, 'models/MultinomialNB.pkl') \n",
    "\n",
    "# Evaluate model performance with log loss\n",
    "score = log_loss(y_test, mnb.predict_proba(X_test))\n",
    "\n",
    "print \"model (log loss): \", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fitting a multilayer perceptron model\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# persist model\n",
    "joblib.dump(mlp, 'models/MLPClassifier.pkl') \n",
    "\n",
    "# Evaluate model performance with log loss\n",
    "score = log_loss(y_test, mlp.predict_proba(X_test))\n",
    "\n",
    "print \"model (log loss): \", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fitting a keras model\n",
    "kc = KerasClassifier(build_fn=create_model, batch_size=128,\n",
    "                     nb_epoch=10, verbose=0)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "param_grid = {'nlayers'     : 2,\n",
    "              'nneurons'    : 10,\n",
    "              'l2_norm'     : 0.01,\n",
    "              'dropout_rate': 0.1,\n",
    "              'input_dim'   : input_dim\n",
    "}\n",
    "\n",
    "kc.set_params(**param_grid)\n",
    "\n",
    "kc.fit(X_train.todense(), y_train.values)\n",
    "\n",
    "# persist model\n",
    "# save a Keras model into a single HDF5 file which will contain:\n",
    "# - the architecture of the model, allowing to re-create the model\n",
    "# - the weights of the model\n",
    "# - the training configuration (loss, optimizer)\n",
    "# - the state of the optimizer, allowing to resume training exactly \n",
    "#   where you left off\n",
    "kc.model.save('models/KerasClassifier.h5')\n",
    "\n",
    "#model = load_model('models/KerasClassifier.h5')\n",
    "\n",
    "#model.predict(X_test.todense())\n",
    "\n",
    "# # saving model architecture\n",
    "# json_model = kc.model.to_json()\n",
    "# open('models/KerasClassifier_architecture.json', 'w').write(json_model)\n",
    "\n",
    "# # saving weights\n",
    "# kc.model.save_weights('models/KerasClassifier_weights.h5', overwrite=True)\n",
    "\n",
    "# # loading model\n",
    "# model = model_from_json(open('models/KerasClassifier_architecture.json').read())\n",
    "# model.load_weights('models/KerasClassifier_weights.h5')\n",
    "\n",
    "# # dont forget to compile your model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Evaluate model performance with log loss\n",
    "print \"model (log loss): \", log_loss(y_test, kc.predict_proba(X_test.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next metric we investigate is AUC that can be obtained by plotting the **ROC** curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot AUC for ROC curve for several classifiers out-of-the-box\n",
    "\n",
    "# prepare models: create a mapping of ML classifier name to algorithm\n",
    "pipeline_search = collections.OrderedDict()\n",
    "\n",
    "pipeline_search['LogisticRegression'] = make_pipeline(None,\n",
    "        LogisticRegression(penalty='l2', C=1))\n",
    "\n",
    "pipeline_search['RandomForestClassifier'] = make_pipeline(None,\n",
    "        RandomForestClassifier(n_estimators=10, max_depth=5, min_samples_leaf=10))\n",
    "\n",
    "pipeline_search['GradientBoostingClassifier'] = make_pipeline(None,\n",
    "        GradientBoostingClassifier(n_estimators=10, max_depth=4, learning_rate=0.2,\n",
    "                                   min_samples_leaf=10))\n",
    "\n",
    "pipeline_search['DecisionTreeClassifier'] = make_pipeline(None,\n",
    "        DecisionTreeClassifier(min_samples_leaf=10))\n",
    "\n",
    "pipeline_search['AdaBoostClassifier'] = make_pipeline(None,\n",
    "        AdaBoostClassifier())\n",
    "\n",
    "pipeline_search['BaggingClassifier'] = make_pipeline(None,\n",
    "        BaggingClassifier(n_estimators=100))\n",
    "\n",
    "pipeline_search['ExtraTreesClassifier'] = make_pipeline(None,\n",
    "        ExtraTreesClassifier(min_samples_leaf=10))\n",
    "\n",
    "pipeline_search['MultinomialNB'] = make_pipeline(None,\n",
    "        MultinomialNB())\n",
    "\n",
    "pipeline_search['DummyClassifier'] = make_pipeline(None,\n",
    "        DummyClassifier(strategy='stratified', random_state=seed))\n",
    "\n",
    "pipeline_search['KerasClassifier'] = make_pipeline(None,\n",
    "        KerasClassifier(build_fn=create_model, batch_size=128,\n",
    "                        nb_epoch=10, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Assessing the various classifiers performance\n",
    "\n",
    "plot_roc_curve(pipeline_search, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also explore **Precesion-Recall** plots. Recall is also know as sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Plot precision-recall curve for several classifiers out-of-the-box\n",
    "\n",
    "plot_precision_recall_curve(pipeline_search, \n",
    "                            X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Plot learning curve\n",
    "\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.33, random_state=seed)\n",
    "\n",
    "plot_learning_curve(pipeline_search['LogisticRegression'],\n",
    "                    X_train, y_train, \n",
    "                    ylim=(0.4, 1.01), cv=cv, n_jobs=-1, \n",
    "                    train_sizes=np.linspace(0.1, 1.0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# paramter grid\n",
    "name = 'GradientBoostingClassifier'\n",
    "\n",
    "param_grid = {name.lower()+'__learning_rate': [0.2, 1.0],\n",
    "              name.lower()+'__max_depth': [1, 4]}\n",
    "\n",
    "# list of models\n",
    "models = model_grid_setup(pipeline_search[name],\n",
    "                          X_train, y_train, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "name = 'GradientBoostingClassifier'.lower()\n",
    "\n",
    "param_name = name+'__n_estimators'\n",
    "\n",
    "param_range = np.linspace(1, 10, num=10, dtype=int)\n",
    "\n",
    "plot_validation_curve(models, X_train, y_train, \n",
    "                      param_name, param_grid, param_range,\n",
    "                      scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "name = 'GradientBoostingClassifier'.lower()\n",
    "\n",
    "param_name = name+'__max_depth'\n",
    "\n",
    "param_range = np.linspace(1, 5, num=5, dtype=int)\n",
    "\n",
    "plot_validation_curve(models, X_train, y_train, \n",
    "                      param_name, param_grid, param_range,\n",
    "                      scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting evaluation\n",
    "\n",
    "#### The Kolmogorov-Smirnov statistic\n",
    "\n",
    "We perform a two-sided asymptotic Kolmogorov-Smirnov test in which the null hypothesis stipulates that two independent samples are drawn from the same continuous parent distribution. If the K-S statistic is small or the p-value is high, then we cannot reject the hypothesis that the distributions of the two samples come from the same parent distribution. \n",
    "\n",
    "In essense, we reject the hypothesis that the two distributions are drawn from the same parent distribution when $D_{n_s,\\ n'_s} > D^{critical}_{n_s,\\ n'_s}$ or fail to reject that hypothesis otherwise.\n",
    "\n",
    "${\\displaystyle D^{critical}_{n,\\ n'} = c(\\alpha ){\\sqrt {\\frac {n + n'}{n \\cdot n'}}}}$, where $n$ and $n'$ are the sizes of two samples, respectively, and\n",
    "\n",
    "${\\displaystyle c\\left(\\alpha \\right) = {\\sqrt {-{\\frac {1}{2}}\\ln \\left({\\frac {\\alpha }{2}}\\right)}}}$\n",
    "\n",
    "We reject the null hypothesis at the 95% level (i.e. $\\alpha=0.05$) which corresponds to $c(0.05) = 1.36$.\n",
    "\n",
    "For signal distributions,\n",
    "\n",
    "${\\displaystyle D^{critical}_{n_s,\\ n'_s} = c(\\alpha ){\\sqrt {\\frac {n_s + n'_s}{n_s \\cdot n'_s}}}}$, where $n_s$ and $n'_s$ are the sizes of training and test signal samples, respectively.\n",
    "\n",
    "For background distributions,\n",
    "\n",
    "${\\displaystyle D^{critical}_{n_b,\\ n'_b} = c(\\alpha ){\\sqrt {\\frac {n_b + n'_b}{n_b \\cdot n'_b}}}}$, where $n_b$ and $n'_b$ are the sizes of training and test background samples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Overfitting evaluation\n",
    "\n",
    "# plot overfitting plot\n",
    "plot_overfitting(pipeline_search['LogisticRegression'],\n",
    "                 X_train, X_test, y_train, y_test, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Uncalibrated model predictions\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "plot_calibration_curve(pipeline_search['LogisticRegression'],\n",
    "                       X_train, X_test, y_train, y_test, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Having completed a comprehensive analysis on the click-through-rate data of TripleLift we conclude that the Logistic Regression Classifier gives the best predictive modeling. Studies were conducted to confirm that the model was not overfitting the data. Further studies can be performed to improve and optimise the modelling, such as using nested k-fold cross validation to obtain the best set of hyper-parameters values, well as to evaluate the final model generalizability to new data to predict on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Apply the random over-sampling\n",
    "\n",
    "ros = RandomOverSampler(random_state=seed)\n",
    "X_overresampled, y_overresampled = ros.fit_sample(df_X, df_y)\n",
    "\n",
    "# Apply the random under-sampling\n",
    "rus = RandomUnderSampler(random_state=seed)\n",
    "X_underresampled, y_underresampled = rus.fit_sample(df_X, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ## Fitting a k nearest neighbor model\n",
    "# kn = KNeighborsClassifier()\n",
    "# kn.fit(X_train, y_train)\n",
    "\n",
    "# # persist model\n",
    "# joblib.dump(kn, 'models/KNeighborsClassifier.pkl') \n",
    "\n",
    "# # Evaluate model performance with log loss\n",
    "# score = log_loss(y_test, kn.predict_proba(X_test))\n",
    "\n",
    "# print \"model (log loss): \", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ## Fitting a support vector classifier model\n",
    "# sv = SVC(class_weight='balanced')\n",
    "# sv.fit(X_train, y_train)\n",
    "\n",
    "# # persist model\n",
    "# joblib.dump(sv, 'models/SVC.pkl') \n",
    "\n",
    "# # Evaluate model performance with log loss\n",
    "# score = log_loss(y_test, sv.predict_proba(X_test))\n",
    "\n",
    "# print \"model (log loss): \", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ## Fitting a linear discriminant analysis model\n",
    "# ld = LinearDiscriminantAnalysis()\n",
    "# ld.fit(X_train, y_train)\n",
    "\n",
    "# # persist model\n",
    "# joblib.dump(ld, 'models/LinearDiscriminantAnalysis.pkl') \n",
    "\n",
    "# # Evaluate model performance with log loss\n",
    "# score = log_loss(y_test, ld.predict_proba(X_test))\n",
    "\n",
    "# print \"model (log loss): \", score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
